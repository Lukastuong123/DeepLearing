{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN&GRU-basic.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1pHMK3TpdtC"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.pyplot import xticks\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,Embedding,Activation,Dropout\n",
        "from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM\n",
        "from keras.layers import Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYqqOf19pgjW"
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l48XEuopgoG"
      },
      "source": [
        "from google.colab import drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrptlPR0pgr7"
      },
      "source": [
        "drive.mount('/content/gdrive',force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5V1wwZpgvu"
      },
      "source": [
        "path = 'gdrive/My Drive/grrr/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPcKt5-8pgzE"
      },
      "source": [
        "d0 = pd.read_csv(path+\"final_tweets.csv\",encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnluuL1kpumi"
      },
      "source": [
        "d0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6l8voGiPFVD"
      },
      "source": [
        "d0.rename(columns = {'Cleaned Tweet 1':'text'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpphnhNGc4Ea"
      },
      "source": [
        "d0 = d0[['text', 'target']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OljLekQhOgWg"
      },
      "source": [
        "d0.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whKKYImHN9nN"
      },
      "source": [
        "from collections import Counter\n",
        "def create_vocab(df):\n",
        "    vocab = Counter()\n",
        "    for i in range(df.shape[0]):\n",
        "        vocab.update(df.text[i].split())\n",
        "    return(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLB1zRNIfSQL"
      },
      "source": [
        "d0[[\"text\"]] = d0[[\"text\"]].astype(str) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vya3tXSQgFoR"
      },
      "source": [
        "d0.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izewFDNDN9rW"
      },
      "source": [
        "\n",
        "# call vocabulary creation function on master dataset\n",
        "vocab = create_vocab(d0)\n",
        "\n",
        "# lets check the no. of words in the vocabulary\n",
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbtG73JN9vP"
      },
      "source": [
        "# lets check the most common 50 words in the vocabulary\n",
        "vocab.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9fER8b7qcwA"
      },
      "source": [
        "# create the final vocab by considering words with more than one occurence\n",
        "final_vocab = []\n",
        "min_occur = 2\n",
        "for k,v in vocab.items():\n",
        "    if v >= min_occur:\n",
        "        final_vocab.append(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1Ifk7OMqc0x"
      },
      "source": [
        "# lets check the no. of the words in the final vocabulary\n",
        "vocab_size = len(final_vocab)\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef0IbfbNqc5A"
      },
      "source": [
        "# function to filter the dataset, keep only words which are present in the vocab\n",
        "def filter(tweet):\n",
        "    sentence = \"\"\n",
        "    for word in tweet.split():  \n",
        "        if word in final_vocab:\n",
        "            sentence = sentence + word + ' '\n",
        "    return(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ0aoPk-qc9s"
      },
      "source": [
        "# apply filter function on the train and test datasets\n",
        "d0['text'] = d0['text'].apply(lambda s : filter(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jXbcVDNqdBh"
      },
      "source": [
        "# the different units into which you can break down text (words, characters, or n-grams) are called tokens, \n",
        "# and breaking text into such tokens is called tokenization, this can be achieved using Tokenizer in Keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    # num_words = vocab_size will create a tokenizer,configured to only take into account the vocab_size(6025)\n",
        "    tokenizer = Tokenizer(num_words=vocab_size)\n",
        "    # Build th word index, Turns strings into lists of integer indices\n",
        "    tokenizer.fit_on_texts(lines) \n",
        "    return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjOVGx5a44OU"
      },
      "source": [
        "# function to calculate f1 score for each epoch\n",
        "import keras.backend as K\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoadKRuguRzq"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "# The Embedding layer takes at least two arguments: the number of possible tokens (here, 5,000: 1 + maximum word index)\n",
        "#and the dimensionality of the embeddings (here, 64).\n",
        "#embedding_layer = Embedding(5000, 64)\n",
        "# Number of words to consider as features\n",
        "max_features = vocab_size\n",
        "\n",
        "# Cuts off the text after this number of words (among the max_features most common words)\n",
        "maxlen = 100\n",
        "# create and apply tokenizer on the training dataset\n",
        "tokenizer = create_tokenizer(d0.text)\n",
        "from keras import preprocessing\n",
        "# conver text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(d0.text)\n",
        "#print(sequences)\n",
        "# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen), padding shorter sequences with 0s\n",
        "train_text = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ls_kODPvevb"
      },
      "source": [
        "# Test train split \n",
        "X_train, X_test, y_train, y_test = train_test_split(train_text, d0.target, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDXjeJD3voTe"
      },
      "source": [
        "# build the model\n",
        "model = Sequential()\n",
        "# Specifies the maximum input length to the Embedding layer so you can later flatten the embedded inputs. \n",
        "\n",
        "# After the Embedding layer, the activations have shape (samples, maxlen, 8)\n",
        "model.add(Embedding(vocab_size, 8, input_length=maxlen))\n",
        "\n",
        "# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer for classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=[get_f1])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj5K-4OT6hy2"
      },
      "source": [
        "callbacks_list = [\n",
        "EarlyStopping(\n",
        "monitor='get_f1',\n",
        "patience=1,\n",
        "),\n",
        "ModelCheckpoint(filepath=path+'./embd.h5',monitor='val_loss',save_best_only=True)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R2omx9g6tko"
      },
      "source": [
        "# train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "epochs=100,\n",
        "batch_size=32,\n",
        "callbacks=callbacks_list,\n",
        "validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-QMt1q-64kh"
      },
      "source": [
        "# check model performance\n",
        "acc = history.history['get_f1']\n",
        "val_acc = history.history['val_get_f1']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PefxVRHw6_pz"
      },
      "source": [
        "import tensorflow as tf\n",
        "dependencies = {\n",
        "    'get_f1': get_f1\n",
        "}\n",
        "\n",
        "\n",
        "# load the model from disk\n",
        "loaded_model_embd = tf.keras.models.load_model(path+'./embd.h5',custom_objects=dependencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh6jnat_7FkU"
      },
      "source": [
        "# prediction on the test dataset\n",
        "y_pred = loaded_model_embd.predict_classes(X_test)\n",
        "\n",
        "# important metrices\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmE310vG9dmw"
      },
      "source": [
        "#simple rnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RPuY8c--j5i"
      },
      "source": [
        "max_words = 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymXpdQl89dqW"
      },
      "source": [
        "from keras.layers import Embedding, SimpleRNN\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thmuXam39dxK"
      },
      "source": [
        "callbacks_list = [\n",
        "EarlyStopping(\n",
        "monitor='get_f1',\n",
        "patience=1,\n",
        "),\n",
        "ModelCheckpoint(filepath=path+'./SRNN.h5',monitor='val_loss',save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[get_f1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qPylmxa-5qc"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "epochs=100,\n",
        "batch_size=128,\n",
        "callbacks=callbacks_list,\n",
        "validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y62PZ0w_-ibs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGTbUJLw9d1v"
      },
      "source": [
        "# check model performance\n",
        "acc = history.history['get_f1']\n",
        "val_acc = history.history['val_get_f1']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8PjjIlhFlSL"
      },
      "source": [
        "dependencies = {\n",
        "    'get_f1': get_f1\n",
        "}\n",
        "\n",
        "\n",
        "# load the model from disk\n",
        "loaded_model_SRNN = keras.models.load_model(path+'./SRNN.h5',custom_objects=dependencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZf18bVoFlea"
      },
      "source": [
        "#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\n",
        "y_pred = loaded_model_SRNN.predict_classes(X_test)\n",
        "\n",
        "# important metrices\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8jOTWOA_hzc"
      },
      "source": [
        "#gru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDSKVhF0_jkq"
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(GRU(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=[get_f1])\n",
        "callbacks_list = [\n",
        "EarlyStopping(\n",
        "monitor='get_f1',\n",
        "patience=1,\n",
        "),\n",
        "ModelCheckpoint(filepath=path+'./GRU.h5',monitor='val_loss',save_best_only=True)\n",
        "]\n",
        "history = model.fit(X_train, y_train,\n",
        "epochs=100,\n",
        "batch_size=128,\n",
        "callbacks=callbacks_list,\n",
        "validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3FX2La7_jp1"
      },
      "source": [
        "# check model performance\n",
        "acc = history.history['get_f1']\n",
        "val_acc = history.history['val_get_f1']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNDWZ8OR_jt7"
      },
      "source": [
        "dependencies = {\n",
        "    'get_f1': get_f1\n",
        "}\n",
        "\n",
        "# load the model from disk\n",
        "loaded_model_GRU = tf.keras.models.load_model(path+'./GRU.h5',custom_objects=dependencies)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkSrQ1Mc9d41"
      },
      "source": [
        "# prediction on the test dataset\n",
        "#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\n",
        "y_pred = loaded_model_GRU.predict_classes(X_test)\n",
        "\n",
        "# important metrices\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}